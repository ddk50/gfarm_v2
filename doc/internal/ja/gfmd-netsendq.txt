非同期要求の場合、要求を発生する側は、コスト負担なしに大量の要求を出すことが
でき、要求を処理する側に過大な負荷をかけ、問題を発生させることになる。

例:
https://sourceforge.net/apps/trac/gfarm/ticket/76
https://sourceforge.net/apps/trac/gfarm/ticket/79

このため、非同期要求に関しては流量制御が必須となる。
netsendq は、その流量制御を実現するモジュールである。

なお、開発作業の都合上、送信処理すべてを netsendq で管理することはしない。
netsendq で管理されてない送信処理は、流量制限なしで行なわれることになる。
逆の言い方をすると、流量制御の必要性の高い送信処理から順に netsendq 化する。

以下のようなポリシーで制御する。
・in-flight な RPC の総数は、流量管理する。
・流量管理は、RPC の種別ごとに行なう。
　特定の RPC が資源を占有するのを防ぐため。

なお、以下では
	gfsd に対して非同期RPCを発行後、返答の返ってないもの
を、in-flight な RPC と表現する。


■ キュー一般

	・abstract_host 毎に、要素種別の数だけキューを作る。
	  (abstract_host::sendq->workqs[type])
	  要素がこの種別キューから外れるのは、送信に対応する応答受信時、
	  ないしエラー発生時。

	・種別キュー毎に in-flight な RPC 数を管理し、
	  その上限値 (window_size) を定め制限する。

	・dead_file_copy の場合、コネクション切断・再開後の再送があるため、
	  送信しても返答を受信するまでは種別キューから　外さない。このため、
	　種別キューの先頭 (GFARM_HCIRCLEQ_FIRST(workq->q, workq_entries))
	　と次に送信する先頭 (workq->next) という2つの先頭ポインタを持つ。

	・window_size 的に送信可能になったら、abstract_host 毎に存在する
	  送信可能キュー (abstract_host::sendq->readyq) に繋ぐ。
	  readyq に繋いでも、workq からは削除しないので注意すること。

	・

	※ 要素の種類別にキューを作るか？１つにするか？
		要素の種類別にキューを作る。
		キューの種別は、優先度順に並べる。
		優先度の高い要素に妨げられて、優先度の低い要素が
		送信されない状況が心配だが、
		種別キュー (abstract_host::sendq->workqs[type]) とは別に、
		種別のない送信可能キュー (abstract_host::sendq->readyq) があり、
		種別のウィンドウが開いている限り、送信可能キューには
		時間順序通り入るので、おそらく問題ない。	   

■ キュー要素の種類別の話

　 プロトコル名と、window size のリミット設定の対応
	プロトコル名			設定
	GFM_PROTO_* reply to gfmd	gfm_proto_reply_to_gfmd_window
	GFM_PROTO_JOURNAL_READY_TO_RECV	不要
	GFM_PROTO_JOURNAL_SEND		gfm_proto_journal_send_window
	GFM_PROTO_REMOTE_PEER_ALLOC	gfm_proto_peer_alloc_window
	GFM_PROTO_REMOTE_PEER_FREE	gfm_proto_peer_free_window
	GFM_PROTO_RPC			gfm_proto_rpc_window
	GFM_PROTO_GFS_RPC		gfm_proto_gfs_rpc_window

	GFM_PROTO_* reply to gfsd	gfm_proto_reply_to_gfsd_window
	GFS_PROTO_STATUS		不要
	GFS_PROTO_FHSTAT		gfs_proto_fhstat_request_window
	GFS_PROTO_FHREMOVE		gfs_proto_fhremove_request_window
	GFS_PROTO_REPLICATION_REQUEST	gfs_proto_replication_request_window
		旧名: simultaneous_replication_receivers

○ GFM_PROTO_JOURNAL_READY_TO_RECV

	接続が切れた場合:
		キューの内容は捨てる

○ GFM_PROTO_JOURNAL_SEND

	接続が切れた場合:
		キューの内容は捨てる

○ GFS_PROTO_STATUS

	接続が切れた場合:
		キューの内容は捨てる

○ GFS_PROTO_FHSTAT

	接続が切れた場合:
		キューの内容は捨てる(?)

○ GFS_PROTO_FHREMOVE

	接続が切れた場合:
		キューの内容は保持

	レプリカ数が足りない場合には、dead_file_copy は存在しても
	実際に、それを gfsd へは送らない。
	これは、送信用キューとは別の単一のキュー kept に繋いで管理する。

	・ホストが up/down した場合には、キューのスキャンが必要
		up の場合:
			生きているレプリカ数が十分になったため、
			kept にあるもののうち、up したのとは別のホスト宛の
			要素の一部が、送信用キューへ移る。
			このため、kept キューを全スキャンする。
		down の場合:
			生きているレプリカ数が不足するため、
			downしたのとは別のgfsd向けキューにあるものの一部が、
			消すべきではなくなる…
			この場合、全 dead_file_copy をスキャンする必要がでる。
			XXX 今回の実装では、このケースを無視している。

○ GFS_PROTO_REPLICATION_REQUEST

	世代更新が終るまでは、複製要求を出せない。
	→ https://sourceforge.net/apps/trac/gfarm/ticket/184
	このため、それまでは struct inode_activity に保持しておき、
	世代更新完了後、送信用キューに移す。

	接続が切れた場合:
		ユーザーからの{src+dst ホスト指定,dstのみ指定}での要求
			dst 側が切れたわけで、諦める。
		既存複製の更新のための要求
			→複製数に関する制約を満たすためのスケジュール
			　XXX 未実装
		複製数に関する制約を満たすための要求
			→制約条件に従い、再度スケジュール
			　XXX 未実装

■ 書換え前のスレッド構造

○ back channel 関連

- FHSTAT 要求
  使ってない

- FHREMOVE 要求
  送信: remover() スレッドが単一のキューから抜き出して、
        送信スレッドプールに対し、thrpool_add_job。
  受信: 受信スレッドプール下のスレッドが、removal_finishedq_enqueue で
	キューに加える。
	独立した removal_finalizer() スレッドが、このキューから取り出して
	処理する。

- STATUS 要求
  送信: (最初の1回目を除き) callout() で送信スレッドプールを使い、定期的に
	スケジュール。
  受信: 受信スレッドプール下のスレッドが、直接処理する。

- REPLICATION_REQUEST 要求
  送信: クライアントに対する foreground 処理の中で、送信スレッドプール
	に対し thrpool_add_job
  受信: 受信スレッドプール下のスレッドが、直接処理する。

- GFM_PROTO_REPLICATION_RESULT 返答
  送信: 受信スレッドプール下のスレッドが、直接処理する。

○ gfmd channel 関連

書き換え前

- gfmdc_send_thread_pool
	作ってるが、使ってない。

- gfmdc_journal_asyncsend_thread()
	async metadata replication は、すべてここで行なっている。
	gfmd.c:start_gfmdc_threads() で生成。
	全ホストに対して gfmdc_journal_asyncsend_each_mdhost() を
	呼び出してから 0.5秒スリープする…という処理を無限ループする。

- gfmdc_journal_asyncsend_each_mdhost()
	引数に渡されたmdhostが自分でも同期メタデータレプリケーションでも
	なければ、gfmdc_journal_asyncsend() を呼び出す。

- gfmdc_journal_asyncsend() 

■ 書換え後のスレッド構造

3種類のキューを設ける。

・abstract_host 毎に、NETSENDQ_TYPE_GF?_PROTO_NUM_TYPES 個ずつ存在する
  abstract_host::sendq->workqs[type]
	送信可能待ちになってから、受信完了されるまで、要求を保持するキュー。
	FHREMOVE の場合、ホストが down しても、リトライを行なうため、
	このキューに保持し続ける。

・abstract_host 毎に、1つずつ存在する abstract_host::sendq->readyq
	送信スレッドの起動を担当するスレッドである netsendq_manager に
	渡す要求のキュー

・back_channel 用に全体で1つ、gfmd_channel 用に全体で一つ、合計2つだけ存在する
  netsenq_manager::hostq
	netsendq_send_manager() が監視しているのは、これ。
	abstract_host::sendq->readyq にエントリが存在しているホストのリスト。
	

※ GFS_PROTO_FHREMOVE のみ、netsendq_type::flags に
　 NETSENDQ_FLAG_QUEUEABLE_IF_DOWN を設定しておく。
　 なお FHREMOVE であっても、一度 abstract_host::sendq->readyq に移った後は、
　 ホストが down したら abstract_host::sendq->workqs[type] から外れる。
　 外れた要素は、FHREMOVE 用の finalize 関数で、再び種別キューに追加される。
　 このようにしている理由は、workq->next の管理を単純化するためと、
　 種別キューに再追加する条件を細かく制御するため。
　 （フレームワーク側に、細かい条件を書いてしまうと、融通が効かない）

※ GFS_PROTO_FHREMOVE のみ、netsendq_type::flags に
　 NETSENDQ_FLAG_PRIOR_ONE_SHOT を設定しておく。

送信側:

(1) 元々の送信要求側スレッドは、以下を行なう。

(1.1) abstract_host 毎にある、
   要求の種類別キュー abstract_host::sendq->workqs[type] に要求を追加する。
   ただし、NETSENDQ_FLAG_QUEUEABLE_IF_DOWN 以外は、その abstract_host が
   down していたら、キューに入れずにエラーを返す。

(1.2) (1.1) で要求の種類別キュー abstract_host::sendq->workqs[type] に
  追加した結果、ウィンドウ制御的な意味で送信可能だった場合は、
  さらに abstract_host 毎にある、送信可能キュー abstract_host::sendq->readyq に
  も追加する。
  このとき、NETSENDQ_FLAG_PRIOR_ONE_SHOT であれば、送信可能キューの先頭に
  加える。

(1.3) 元々の要求側のスレッドは、この abstract_host が、netsendq_manager 
　の監視する、送信待ちホスト・キュー netsendq_manager::hostq に登録されている
  かどうか調べ、登録されていなければ、netsendq_manager::hostq に追加する。
  これで、元々の要求側のスレッドは送信処理を完了する。
  これにより、以下を保証している。
	元々の要求側とは別のスレッド (== netsendq_send_manager()) が、
	thrpool_add_job を行なうため、送信用スレッドプールが満杯でも、
	元々の要求側スレッドが待たされることがない。

(2) netsendq_send_manager() は独立したスレッドであり、送信待ちホスト・キュー
　以下の点について、netsendq_manager::hostq を監視する。
	・要素が追加される
	・ある宛先について送信が完了し、次の送信が可能になる。
  これにより、送信可能になったホストを検知したら、以下を行なう。
  (注: 検知と同時に、その送信先の状態を、現在送信中 qhost->sending にしている)
  その送信先へ、現在送信中でないことを確認し、
  abstract_host::sendq->readyq から、要求を抜き出す。
  次に、その送信先が up していることを確認し、
  up していれば thrpool_add_job() を行なう。
  その送信先が down している場合は、netsendq_host_is_down_at_entry() を行なう。

(3) 上記 (2) で manager->send_thrpool へ thrpool_add_job() された、
  種別ごとの送信処理関数は、以下を行なう。

(3.1) 宛先ホストへ、送信を行なう

(3.2) netsendq_was_sent_to_host() を呼び出し、
  送信完了を netsendq モジュールへ通知する。
  netsendq_was_sent_to_host() は、
  そのホストを、送信可能状態 (すなわち !qhost->sending) に戻した上、
  条件変数 manager->sendable を介して、
  そのことを netsendq_send_manager() スレッドに通知する。
  netsendq_send_manager() は、これにより起床する。

(4) 送信に失敗したり、あるいは送信に対応する受信処理が完了したら、
  受信の結果ウィンドウが開いたかどうか確認し、
  開いたのであれば、abstract_host::sendq->workqs[type] の次のエントリーに、
  対し、上記(2)以降の処理を行なう。

(5) そのホストが down した場合、
  abstract_host::sendq->readyq のエントリーは、すべて削除する。
  abstract_host::sendq->workqs[type] で、まだ送信に至ってないエントリーは、
  netsendq_host_is_down_at_entry() を行なう。

受信側:

(1) giant lock のような、大きなロックが不要であれば、受信スレッドプール下
  のスレッドで直接処理する。

(2) giant lock が必要な処理、すなわち FHREMOVE や REPLICATION_RESULT などの
  処理については、netsendq_finalizer 用のキューに追加する。

(3) netsendq_finalizer は独立したスレッドであり、netsendq_finalizer 専用の
  キューに要素が加えられたら、自スレッドで片付け処理を行なう。
  これにより、以下を保証している。
	受信用スレッドプールを、giant lock 等で封鎖しない。
	これにより、デッドロック等を避ける。
	XXX 送信処理や受信処理とことなり、netsendq_finalizer はシングル
	    スレッドの実装となっている。マルチスレッド化すべきか？

■ 外部インターフェース

▼ 外部インターフェース一覧

モジュール初期化	netsendq_manager_new
キュー初期化		netsendq_new
queue entry初期化	netsendq_entry_init
queue entry送信完了通知	netsendq_entry_was_sent		※ エラーでも呼ぶこと
送信後・受信完了通知	netsendq_remove_entry		※ エラーでも呼ぶこと
queue entry解放		netsendq_entry_destroy
現在送信がビジーか	netsendq_readyq_is_full		※ replica_check 機能用
キューに追加		netsendq_add_entry
ホスト削除通知		netsendq_host_remove
ホストdown通知		netsendq_host_becomes_down
ホストup通知		netsendq_host_becomes_up

▼ 利用例

○ netsendq_manager_new()

gfmd プロセスは、2 つの struct netsendq_manager を保持する。
一つは、back_channel_send_manager、
もう一つは gfmd_channel_send_manager（ただし、こちらは未実装）。

struct netsendq_manager は、以下を指定して生成する。
・送信キューに繋ぐエントリの種別を表す配列の長さ。
  以下の例では、NETSENDQ_TYPE_GFS_PROTO_NUM_TYPES。
・送信キューに繋ぐエントリの種別を表す配列。
  以下の例では、back_channel_queue_types。
  配列の要素は、送信キューに繋ぐ RPC の種類ごとの、以下の要素
	- 送信処理関数
	- 送信完了コールバック関数
	- 同時に送信して良いキュー・エントリ数 (window size)
	- NETSENDQ_FLAG_*
	- 種別を表す整数値
・送信処理に用いるスレッドプールの最大スレッド数。
  以下の例では、gfarm_metadb_thread_pool_size。
・送信処理に用いるスレッドプールのスレッド投入待ちキューの最大長。
  以下の例では、gfarm_metadb_thread_queue_length。
・diag 文字列
  以下の例では、"send queue to gfsd"。

例:
#define NETSENDQ_TYPE_GFS_PROTO_STATUS			0
#define NETSENDQ_TYPE_GFS_PROTO_FHREMOVE		1
#define NETSENDQ_TYPE_GFS_PROTO_REPLICATION_REQUEST	2
#define NETSENDQ_TYPE_GFS_PROTO_NUM_TYPES		3

static void *gfs_client_status_request(void *arg);
static void gfs_client_status_finalize(struct netsendq_entry *qentryp);

struct netsendq_type gfs_proto_status_queue = {
	gfs_client_status_request,
	gfs_client_status_finalize,
	1,
	NETSENDQ_FLAG_PRIOR_ONE_SHOT,
	NETSENDQ_TYPE_GFS_PROTO_STATUS
};

static void *gfs_client_fhremove_request(void *arg);
static void handle_removal_result(struct netsendq_entry *qentryp);

struct netsendq_type gfs_proto_fhremove_queue = {
	gfs_client_fhremove_request,
	handle_removal_result,
	gfs_proto_fhremove_request_window,
	0,
	NETSENDQ_TYPE_GFS_PROTO_FHREMOVE
};

static void *gfs_client_replication_request_request(void *arg);
static void handle_file_replication_result(struct netsendq_entry *qentryp);

struct netsendq_type gfs_proto_replication_request_queue = {
	gfs_client_replication_request_request,
	handle_file_replication_result,
	gfs_proto_replication_request_window,
	0,
	NETSENDQ_TYPE_GFS_PROTO_REPLICATION_REQUEST
};

const struct netsendq_type *back_channel_queue_types[NETSENDQ_TYPE_GFS_PROTO_NUM_TYPES] = {
	&gfs_proto_status_queue,
	&gfs_proto_fhremove_queue,
	&gfs_proto_replication_request_queue,
};

	back_channel_send_manager = netsendq_manager_new(
	    NETSENDQ_TYPE_GFS_PROTO_NUM_TYPES, back_channel_queue_types,
	    gfarm_metadb_thread_pool_size, gfarm_metadb_job_queue_length,
	    "send queue to gfsd");

○ netsendq_new()

abstract_host_init() で、その通信相手毎にキューを作成しているので、
通常は意識する必要はない。

○ netsendq を用いる、具体的なデータ構造の例

struct dead_file_copy {
	struct netsendq_entry qentry; /* must be first member */

	gfarm_ino_t inum;
	gfarm_uint64_t igen;
};
… 構造体の先頭に netsendq_entry を置く

static struct dead_file_copy *
dead_file_copy_alloc(gfarm_ino_t inum, gfarm_uint64_t igen, struct host *host)
{
	struct dead_file_copy *dfc;

	GFARM_MALLOC(dfc);
	netsendq_entry_init(&dfc->qentry, &gfs_proto_fhremove_queue);

	dfc->qentry.abhost = host_to_abstract_host(host);
	dfc->inum = inum;
	dfc->igen = igen;

	return (dfc);
}
… netsendq_entry_init() で初期化する

void
dead_file_copy_schedule_removal(struct dead_file_copy *dfc)
{
	struct netsendq *qhost =
	    abstract_host_get_sendq(dfc->qentry.abhost);
	static const char diag[] = "dead_file_copy_schedule_removal";

	/* should success always, because of NETSENDQ_FLAG_QUEUEABLE_IF_DOWN */
	(void)netsendq_add_entry(qhost, &dfc->qentry, 0);
}
… netsendq_add_entry で netsendq に enqueue する
